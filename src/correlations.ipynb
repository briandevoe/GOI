{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3375d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading indicators...\n",
      "  NDVI: tract_ndvi_summer_2021_2025_summary.csv  col=mean_ndvi  rows=85,187\n",
      "  EVI : tract_evi_summer_2021_2025_summary.csv   col=mean_evi   rows=85,187\n",
      "  TCC : tract_tcc_nlcd_2021_2023_summary.csv   col=tcc_mean_2021_2023   rows=85,187\n",
      "Biophysical merged rows: 85,187\n",
      "\n",
      "Loading NLCD + PLACES...\n",
      "Using state col: StateAbbr\n",
      "No RUCA detected in PLACES (pooled models only).\n",
      "\n",
      "NLCD columns selected:\n",
      "  - pct_deciduous_forest\n",
      "  - pct_evergreen_forest\n",
      "  - pct_mixed_forest\n",
      "  - pct_shrub_scrub\n",
      "  - pct_grassland_herbaceous\n",
      "  - pct_emergent_herbaceous_wetlands\n",
      "  - pct_woody_wetlands\n",
      "  - pct_pasture_hay\n",
      "  - pct_developed_open_space\n",
      "\n",
      "Full merged rows: 85,187\n",
      "Rows with PLACES present (OBESITY_CrudePrev): 77,939\n",
      "\n",
      "==========================================================================================\n",
      "Scaled diagnostics (first 12 scaled cols)\n",
      "==========================================================================================\n",
      "- mean_ndvi_01: n=83,509  min=0.0000  p1=0.0000  p50=0.6124  p99=1.0000  max=1.0000\n",
      "- mean_evi_01: n=83,509  min=0.0000  p1=0.0000  p50=0.5277  p99=1.0000  max=1.0000\n",
      "- tcc_mean_2021_2023_01: n=83,509  min=0.0000  p1=0.0000  p50=0.2372  p99=1.0000  max=1.0000\n",
      "- pct_deciduous_forest_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0005  p99=1.0000  max=1.0000\n",
      "- pct_evergreen_forest_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=1.0000  max=1.0000\n",
      "- pct_mixed_forest_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=1.0000  max=1.0000\n",
      "- pct_shrub_scrub_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=1.0000  max=1.0000\n",
      "- pct_grassland_herbaceous_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=1.0000  max=1.0000\n",
      "- pct_emergent_herbaceous_wetlands_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=0.9998  max=1.0000\n",
      "- pct_woody_wetlands_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0034  p99=1.0000  max=1.0000\n",
      "- pct_pasture_hay_01: n=83,509  min=0.0000  p1=0.0000  p50=0.0000  p99=1.0000  max=1.0000\n",
      "- pct_developed_open_space_01: n=83,509  min=0.0000  p1=0.0000  p50=0.1636  p99=1.0000  max=1.0000\n",
      "\n",
      "PCA summary:\n",
      "  N used: 83,509\n",
      "  Explained variance ratios: [0.34629463 0.127619   0.10584582]\n",
      "\n",
      "  PC1 loadings (sorted):\n",
      "mean_ndvi_01                           0.460371\n",
      "mean_evi_01                            0.451069\n",
      "tcc_mean_2021_2023_01                  0.444449\n",
      "pct_deciduous_forest_01                0.345119\n",
      "pct_mixed_forest_01                    0.312656\n",
      "pct_pasture_hay_01                     0.246732\n",
      "pct_woody_wetlands_01                  0.206314\n",
      "pct_evergreen_forest_01                0.167844\n",
      "pct_developed_open_space_01            0.165669\n",
      "pct_emergent_herbaceous_wetlands_01    0.023441\n",
      "pct_grassland_herbaceous_01           -0.033993\n",
      "pct_shrub_scrub_01                    -0.097126\n",
      "\n",
      "Running validation regressions...\n",
      "\n",
      "==========================================================================================\n",
      "TOPLINE RESULTS (first 120 rows)\n",
      "==========================================================================================\n",
      "                     index             model ruca_level           outcome   estimate  std_error          t             p     n       r2\n",
      "                  green_eq    pooled_stateFE       None     LPA_CrudePrev  -6.804213   0.167179 -40.700205  0.000000e+00 77337 0.177694\n",
      "                  green_eq pooled_unadjusted       None     LPA_CrudePrev  -0.684953   0.119045  -5.753717  8.730247e-09 77337 0.000413\n",
      "green_nlcd_balanced_canopy    pooled_stateFE       None     LPA_CrudePrev -13.315231   0.328855 -40.489611  0.000000e+00 77337 0.175964\n",
      "green_nlcd_balanced_canopy pooled_unadjusted       None     LPA_CrudePrev  -2.884705   0.275880 -10.456370  1.370048e-25 77337 0.001377\n",
      "   green_nlcd_bio_dominant    pooled_stateFE       None     LPA_CrudePrev -10.459342   0.252040 -41.498786  0.000000e+00 77337 0.177804\n",
      "   green_nlcd_bio_dominant pooled_unadjusted       None     LPA_CrudePrev  -1.596904   0.192784  -8.283363  1.197449e-16 77337 0.000860\n",
      "green_nlcd_canopy_weighted    pooled_stateFE       None     LPA_CrudePrev -12.729729   0.301679 -42.196276  0.000000e+00 77337 0.177599\n",
      "green_nlcd_canopy_weighted pooled_unadjusted       None     LPA_CrudePrev  -3.001205   0.247963 -12.103451  1.012623e-33 77337 0.001840\n",
      "   green_nlcd_conservative    pooled_stateFE       None     LPA_CrudePrev  -9.774616   0.235875 -41.439859  0.000000e+00 77337 0.177907\n",
      "   green_nlcd_conservative pooled_unadjusted       None     LPA_CrudePrev  -1.378719   0.177538  -7.765763  8.115507e-15 77337 0.000755\n",
      "    green_nlcd_eq_expanded    pooled_stateFE       None     LPA_CrudePrev -13.217636   0.322966 -40.925781  0.000000e+00 77337 0.176379\n",
      "    green_nlcd_eq_expanded pooled_unadjusted       None     LPA_CrudePrev  -2.925827   0.269603 -10.852366  1.943296e-27 77337 0.001483\n",
      "  green_nlcd_nlcd_dominant    pooled_stateFE       None     LPA_CrudePrev -14.589447   0.374610 -38.945721  0.000000e+00 77337 0.173854\n",
      "  green_nlcd_nlcd_dominant pooled_unadjusted       None     LPA_CrudePrev  -4.474842   0.339948 -13.163319  1.426785e-39 77337 0.002182\n",
      "    green_nlcd_vi_weighted    pooled_stateFE       None     LPA_CrudePrev -11.729988   0.290458 -40.384475  0.000000e+00 77337 0.176480\n",
      "    green_nlcd_vi_weighted pooled_unadjusted       None     LPA_CrudePrev  -1.801403   0.229744  -7.840916  4.472725e-15 77337 0.000772\n",
      "             green_pca_pc1    pooled_stateFE       None     LPA_CrudePrev  -0.705774   0.018468 -38.216103  0.000000e+00 77337 0.173584\n",
      "             green_pca_pc1 pooled_unadjusted       None     LPA_CrudePrev  -0.063676   0.014090  -4.519182  6.207902e-06 77337 0.000240\n",
      "                  green_eq    pooled_stateFE       None   MHLTH_CrudePrev  -2.712423   0.064146 -42.285386  0.000000e+00 77337 0.155517\n",
      "                  green_eq pooled_unadjusted       None   MHLTH_CrudePrev  -0.411961   0.044498  -9.257962  2.083711e-20 77337 0.001005\n",
      "green_nlcd_balanced_canopy    pooled_stateFE       None   MHLTH_CrudePrev  -5.627045   0.126894 -44.344529  0.000000e+00 77337 0.156103\n",
      "green_nlcd_balanced_canopy pooled_unadjusted       None   MHLTH_CrudePrev  -1.455592   0.100249 -14.519797  9.077923e-48 77337 0.002362\n",
      "   green_nlcd_bio_dominant    pooled_stateFE       None   MHLTH_CrudePrev  -4.285274   0.096861 -44.241637  0.000000e+00 77337 0.156854\n",
      "   green_nlcd_bio_dominant pooled_unadjusted       None   MHLTH_CrudePrev  -0.854194   0.071018 -12.027775  2.539123e-33 77337 0.001658\n",
      "green_nlcd_canopy_weighted    pooled_stateFE       None   MHLTH_CrudePrev  -5.305687   0.116637 -45.488876  0.000000e+00 77337 0.157413\n",
      "green_nlcd_canopy_weighted pooled_unadjusted       None   MHLTH_CrudePrev  -1.408756   0.090671 -15.536987  1.949350e-54 77337 0.002731\n",
      "   green_nlcd_conservative    pooled_stateFE       None   MHLTH_CrudePrev  -3.982766   0.090598 -43.961064  0.000000e+00 77337 0.156720\n",
      "   green_nlcd_conservative pooled_unadjusted       None   MHLTH_CrudePrev  -0.751897   0.065589 -11.463849  2.004055e-30 77337 0.001513\n",
      "    green_nlcd_eq_expanded    pooled_stateFE       None   MHLTH_CrudePrev  -5.567412   0.124673 -44.655940  0.000000e+00 77337 0.156453\n",
      "    green_nlcd_eq_expanded pooled_unadjusted       None   MHLTH_CrudePrev  -1.449354   0.098102 -14.773971  2.156257e-49 77337 0.002451\n",
      "  green_nlcd_nlcd_dominant    pooled_stateFE       None   MHLTH_CrudePrev  -6.319770   0.145507 -43.432716  0.000000e+00 77337 0.154556\n",
      "  green_nlcd_nlcd_dominant pooled_unadjusted       None   MHLTH_CrudePrev  -2.099429   0.122918 -17.079871  2.095830e-65 77337 0.003236\n",
      "    green_nlcd_vi_weighted    pooled_stateFE       None   MHLTH_CrudePrev  -4.883655   0.111525 -43.789879  0.000000e+00 77337 0.156055\n",
      "    green_nlcd_vi_weighted pooled_unadjusted       None   MHLTH_CrudePrev  -1.012032   0.083964 -12.053206  1.865496e-33 77337 0.001642\n",
      "             green_pca_pc1    pooled_stateFE       None   MHLTH_CrudePrev  -0.287915   0.007161 -40.204072  0.000000e+00 77337 0.151926\n",
      "             green_pca_pc1 pooled_unadjusted       None   MHLTH_CrudePrev  -0.040566   0.005233  -7.752233  9.029047e-15 77337 0.000655\n",
      "                  green_eq    pooled_stateFE       None OBESITY_CrudePrev   0.654667   0.132889   4.926436  8.374306e-07 77337 0.308015\n",
      "                  green_eq pooled_unadjusted       None OBESITY_CrudePrev   7.616417   0.100367  75.885611  0.000000e+00 77337 0.064350\n",
      "green_nlcd_balanced_canopy    pooled_stateFE       None OBESITY_CrudePrev   1.635335   0.265406   6.161644  7.199364e-10 77337 0.308130\n",
      "green_nlcd_balanced_canopy pooled_unadjusted       None OBESITY_CrudePrev  15.372368   0.231933  66.279458  0.000000e+00 77337 0.049333\n",
      "   green_nlcd_bio_dominant    pooled_stateFE       None OBESITY_CrudePrev   1.060788   0.201718   5.258766  1.450256e-07 77337 0.308043\n",
      "   green_nlcd_bio_dominant pooled_unadjusted       None OBESITY_CrudePrev  11.666061   0.162034  71.997595  0.000000e+00 77337 0.057908\n",
      "green_nlcd_canopy_weighted    pooled_stateFE       None OBESITY_CrudePrev   0.713940   0.243577   2.931071  3.377951e-03 77337 0.307860\n",
      "green_nlcd_canopy_weighted pooled_unadjusted       None OBESITY_CrudePrev  12.671242   0.209995  60.340802  0.000000e+00 77337 0.041372\n",
      "   green_nlcd_conservative    pooled_stateFE       None OBESITY_CrudePrev   0.981012   0.188478   5.204918  1.940826e-07 77337 0.308038\n",
      "   green_nlcd_conservative pooled_unadjusted       None OBESITY_CrudePrev  10.898248   0.149262  73.014079  0.000000e+00 77337 0.059523\n",
      "    green_nlcd_eq_expanded    pooled_stateFE       None OBESITY_CrudePrev   1.412034   0.260694   5.416448  6.079469e-08 77337 0.308051\n",
      "    green_nlcd_eq_expanded pooled_unadjusted       None OBESITY_CrudePrev  14.742712   0.227052  64.931004  0.000000e+00 77337 0.047491\n",
      "  green_nlcd_nlcd_dominant    pooled_stateFE       None OBESITY_CrudePrev   1.640661   0.304490   5.388222  7.115808e-08 77337 0.308040\n",
      "  green_nlcd_nlcd_dominant pooled_unadjusted       None OBESITY_CrudePrev  15.826481   0.288884  54.784977  0.000000e+00 77337 0.034435\n",
      "    green_nlcd_vi_weighted    pooled_stateFE       None OBESITY_CrudePrev   1.743615   0.232840   7.488463  6.968474e-14 77337 0.308307\n",
      "    green_nlcd_vi_weighted pooled_unadjusted       None OBESITY_CrudePrev  14.329869   0.191868  74.686202  0.000000e+00 77337 0.061631\n",
      "             green_pca_pc1    pooled_stateFE       None OBESITY_CrudePrev   0.078819   0.014938   5.276305  1.318143e-07 77337 0.308032\n",
      "             green_pca_pc1 pooled_unadjusted       None OBESITY_CrudePrev   0.844003   0.012009  70.278537  0.000000e+00 77337 0.053084\n",
      "                  green_eq    pooled_stateFE       None   PHLTH_CrudePrev  -0.141493   0.069827  -2.026327  4.273131e-02 77337 0.105114\n",
      "                  green_eq pooled_unadjusted       None   PHLTH_CrudePrev   0.454840   0.050215   9.057872  1.330204e-19 77337 0.001135\n",
      "green_nlcd_balanced_canopy    pooled_stateFE       None   PHLTH_CrudePrev   0.899002   0.138143   6.507763  7.627838e-11 77337 0.105583\n",
      "green_nlcd_balanced_canopy pooled_unadjusted       None   PHLTH_CrudePrev   2.014970   0.112167  17.963995  3.729714e-72 77337 0.004193\n",
      "   green_nlcd_bio_dominant    pooled_stateFE       None   PHLTH_CrudePrev   0.196043   0.105515   1.857969  6.317347e-02 77337 0.105104\n",
      "   green_nlcd_bio_dominant pooled_unadjusted       None   PHLTH_CrudePrev   1.044933   0.079998  13.062028  5.426244e-39 77337 0.002298\n",
      "green_nlcd_canopy_weighted    pooled_stateFE       None   PHLTH_CrudePrev   0.451147   0.126882   3.555655  3.770381e-04 77337 0.105217\n",
      "green_nlcd_canopy_weighted pooled_unadjusted       None   PHLTH_CrudePrev   1.524786   0.101346  15.045293  3.706883e-51 77337 0.002964\n",
      "   green_nlcd_conservative    pooled_stateFE       None   PHLTH_CrudePrev   0.104730   0.098684   1.061263  2.885706e-01 77337 0.105074\n",
      "   green_nlcd_conservative pooled_unadjusted       None   PHLTH_CrudePrev   0.904264   0.073937  12.230191  2.144339e-34 77337 0.002027\n",
      "    green_nlcd_eq_expanded    pooled_stateFE       None   PHLTH_CrudePrev   0.790839   0.135710   5.827417  5.629196e-09 77337 0.105480\n",
      "    green_nlcd_eq_expanded pooled_unadjusted       None   PHLTH_CrudePrev   1.897654   0.109748  17.291048  5.494389e-67 77337 0.003892\n",
      "  green_nlcd_nlcd_dominant    pooled_stateFE       None   PHLTH_CrudePrev   1.496361   0.157855   9.479355  2.558589e-21 77337 0.106128\n",
      "  green_nlcd_nlcd_dominant pooled_unadjusted       None   PHLTH_CrudePrev   2.885238   0.135990  21.216498 6.725010e-100 77337 0.005661\n",
      "    green_nlcd_vi_weighted    pooled_stateFE       None   PHLTH_CrudePrev   0.589331   0.121590   4.846887  1.254138e-06 77337 0.105357\n",
      "    green_nlcd_vi_weighted pooled_unadjusted       None   PHLTH_CrudePrev   1.512983   0.094546  16.002534  1.226782e-57 77337 0.003399\n",
      "             green_pca_pc1    pooled_stateFE       None   PHLTH_CrudePrev   0.031777   0.007762   4.094217  4.235981e-05 77337 0.105262\n",
      "             green_pca_pc1 pooled_unadjusted       None   PHLTH_CrudePrev   0.091418   0.005906  15.478474  4.848411e-54 77337 0.003081\n",
      "\n",
      "Saved outputs:\n",
      "  - /mnt/sda-21.8/bdevoe/greenspace/processed/index_validation/tract_indices_merged.csv\n",
      "  - /mnt/sda-21.8/bdevoe/greenspace/processed/index_validation/places_validation_results.csv\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ------------------------------------------------------------\n",
    "# purpose: Build greenspace indices (equal weights, NLCD schemes, PCA)\n",
    "#          and validate against CDC PLACES outcomes (pooled + RUCA strat)\n",
    "# notes:   Python 3.9 compatible; robust scaling + NLCD de-dupe\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG\n",
    "# ============================================================\n",
    "\n",
    "ROOT = Path(\"/mnt/sda-21.8/bdevoe/greenspace\")\n",
    "\n",
    "NDVI_FILE = ROOT / \"processed\" / \"tract_ndvi_summer_2021_2025_summary.csv\"\n",
    "EVI_FILE  = ROOT / \"processed\" / \"tract_evi_summer_2021_2025_summary.csv\"\n",
    "TCC_FILE  = ROOT / \"processed\" / \"tract_tcc_nlcd_2021_2023_summary.csv\"\n",
    "\n",
    "NLCD_FILE   = ROOT / \"processed\" / \"nlcd_2023_tract_fullclass_pct_cb2020_500k_CONUS_READABLE.csv\"\n",
    "PLACES_FILE = ROOT / \"outcomes\" / \"places_new\" / \"places_tracts.csv\"\n",
    "\n",
    "RUCA_FILE = None\n",
    "\n",
    "GEOID_CANDIDATES = [\"GEOID\", \"geoid\", \"TractFIPS\", \"tractfips\", \"FIPS\", \"fips\"]\n",
    "\n",
    "NDVI_COL_CANDIDATES = [\"mean_ndvi\", \"ndvi_mean\", \"ndvi\", \"NDVI\"]\n",
    "EVI_COL_CANDIDATES  = [\"mean_evi\", \"evi_mean\", \"evi\", \"EVI\"]\n",
    "TCC_COL_CANDIDATES  = [\n",
    "    \"tcc_mean_2021_2023\",\n",
    "    \"tcc_2023\",\n",
    "    \"mean_tcc\",\n",
    "    \"tcc_mean\",\n",
    "    \"tcc\",\n",
    "    \"TCC\",\n",
    "    \"tree_canopy\",\n",
    "    \"pct_tree_canopy\",\n",
    "]\n",
    "\n",
    "STATE_COL_CANDIDATES = [\"StateAbbr\", \"state\", \"STATE\", \"stusps\", \"STUSPS\"]\n",
    "RUCA_COL_CANDIDATES  = [\"ruca10\", \"RUCA10\", \"ruca\", \"RUCA\", \"ruca_code\"]\n",
    "\n",
    "PLACES_OUTCOMES = [\n",
    "    \"OBESITY_CrudePrev\",\n",
    "    \"LPA_CrudePrev\",\n",
    "    \"MHLTH_CrudePrev\",\n",
    "    \"PHLTH_CrudePrev\",\n",
    "]\n",
    "\n",
    "WINSORIZE = True\n",
    "WINSOR_Q = (0.01, 0.99)\n",
    "EPS = 1e-12\n",
    "\n",
    "SAVE_OUTPUTS = True\n",
    "OUT_DIR = ROOT / \"processed\" / \"index_validation\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def find_first_existing_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def standardize_geoid(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.str.zfill(11)\n",
    "    return s\n",
    "\n",
    "def winsorize_series(x: pd.Series, q_low: float = 0.01, q_high: float = 0.99) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust winsorization:\n",
    "    - coerce to numeric\n",
    "    - compute quantiles\n",
    "    - use np.clip (avoids pandas alignment / axis errors)\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(x, errors=\"coerce\")\n",
    "    lo = x.quantile(q_low)\n",
    "    hi = x.quantile(q_high)\n",
    "    arr = x.to_numpy(dtype=float)\n",
    "    return pd.Series(np.clip(arr, lo, hi), index=x.index)\n",
    "\n",
    "def minmax_01(x: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(x, errors=\"coerce\").astype(float)\n",
    "    xmin = np.nanmin(x.values)\n",
    "    xmax = np.nanmax(x.values)\n",
    "    if np.isfinite(xmin) and np.isfinite(xmax) and (xmax - xmin) > EPS:\n",
    "        return (x - xmin) / (xmax - xmin)\n",
    "    return pd.Series(np.nan, index=x.index)\n",
    "\n",
    "def scale_01(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in out.columns:\n",
    "            continue\n",
    "        x = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "        if WINSORIZE:\n",
    "            x = winsorize_series(x, WINSOR_Q[0], WINSOR_Q[1])\n",
    "        out[f\"{c}_01\"] = minmax_01(x)\n",
    "    return out\n",
    "\n",
    "def safe_mean(df: pd.DataFrame, cols: List[str], outcol: str) -> pd.DataFrame:\n",
    "    df[outcol] = df[cols].mean(axis=1, skipna=True)\n",
    "    return df\n",
    "\n",
    "def print_basic_diagnostics(df: pd.DataFrame, cols: List[str], title: str) -> None:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(title)\n",
    "    print(\"=\"*90)\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            print(f\"- {c}: MISSING\")\n",
    "            continue\n",
    "        x = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        n = x.notna().sum()\n",
    "        if n == 0:\n",
    "            print(f\"- {c}: all missing\")\n",
    "            continue\n",
    "        print(\n",
    "            f\"- {c}: n={n:,}  \"\n",
    "            f\"min={np.nanmin(x):.4f}  p1={x.quantile(0.01):.4f}  \"\n",
    "            f\"p50={x.quantile(0.50):.4f}  p99={x.quantile(0.99):.4f}  \"\n",
    "            f\"max={np.nanmax(x):.4f}\"\n",
    "        )\n",
    "\n",
    "def load_indicator(file_path: Path, value_candidates: List[str]) -> Tuple[pd.DataFrame, str]:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    geoid_col = find_first_existing_col(df, GEOID_CANDIDATES)\n",
    "    if geoid_col is None:\n",
    "        raise ValueError(f\"No GEOID column found in {file_path.name}. Columns: {list(df.columns)[:50]} ...\")\n",
    "\n",
    "    val_col = find_first_existing_col(df, value_candidates)\n",
    "    if val_col is None:\n",
    "        raise ValueError(f\"No indicator value column found in {file_path.name}. Columns: {list(df.columns)[:50]} ...\")\n",
    "\n",
    "    df = df[[geoid_col, val_col]].copy()\n",
    "    df.rename(columns={geoid_col: \"GEOID\"}, inplace=True)\n",
    "    df[\"GEOID\"] = standardize_geoid(df[\"GEOID\"])\n",
    "    return df, val_col\n",
    "\n",
    "def normalize_weights(w: Dict[str, float]) -> Dict[str, float]:\n",
    "    s = float(sum(w.values()))\n",
    "    if s <= 0:\n",
    "        raise ValueError(\"Weights must sum to > 0.\")\n",
    "    return {k: float(v) / s for k, v in w.items()}\n",
    "\n",
    "def build_weighted_index(df: pd.DataFrame, weights: Dict[str, float], outcol: str) -> pd.DataFrame:\n",
    "    keys = list(weights.keys())\n",
    "    w = np.array([weights[k] for k in keys], dtype=float)\n",
    "\n",
    "    X = df[keys].to_numpy(dtype=float)\n",
    "    mask = np.isfinite(X)\n",
    "\n",
    "    w_mat = np.tile(w, (X.shape[0], 1))\n",
    "    w_mat[~mask] = 0.0\n",
    "\n",
    "    w_sum = w_mat.sum(axis=1)\n",
    "    w_sum = np.where(w_sum > EPS, w_sum, np.nan)\n",
    "\n",
    "    df[outcol] = np.nansum(X * w_mat, axis=1) / w_sum\n",
    "    return df\n",
    "\n",
    "def run_models(\n",
    "    df: pd.DataFrame,\n",
    "    outcome: str,\n",
    "    index_col: str,\n",
    "    state_col: str,\n",
    "    ruca_col: Optional[str],\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    cols = [outcome, index_col, state_col]\n",
    "    if ruca_col:\n",
    "        cols.append(ruca_col)\n",
    "\n",
    "    d = df[cols].copy().dropna(subset=[outcome, index_col, state_col])\n",
    "\n",
    "    m1 = smf.ols(f\"{outcome} ~ {index_col}\", data=d).fit(cov_type=\"HC1\")\n",
    "    results.append((\"pooled_unadjusted\", None, m1))\n",
    "\n",
    "    m2 = smf.ols(f\"{outcome} ~ {index_col} + C({state_col})\", data=d).fit(cov_type=\"HC1\")\n",
    "    results.append((\"pooled_stateFE\", None, m2))\n",
    "\n",
    "    if ruca_col and ruca_col in d.columns:\n",
    "        m3 = smf.ols(f\"{outcome} ~ {index_col} + C({state_col}) + C({ruca_col})\", data=d).fit(cov_type=\"HC1\")\n",
    "        results.append((\"pooled_stateFE_rucaFE\", None, m3))\n",
    "\n",
    "        for r in sorted(d[ruca_col].dropna().unique()):\n",
    "            dr = d[d[ruca_col] == r].copy()\n",
    "            if len(dr) < 200:\n",
    "                continue\n",
    "            ma = smf.ols(f\"{outcome} ~ {index_col}\", data=dr).fit(cov_type=\"HC1\")\n",
    "            mb = smf.ols(f\"{outcome} ~ {index_col} + C({state_col})\", data=dr).fit(cov_type=\"HC1\")\n",
    "            results.append((\"ruca_strat_unadjusted\", r, ma))\n",
    "            results.append((\"ruca_strat_stateFE\", r, mb))\n",
    "\n",
    "    tidy = []\n",
    "    for model_name, ruca_level, model in results:\n",
    "        if index_col not in model.params.index:\n",
    "            continue\n",
    "        tidy.append({\n",
    "            \"index\": index_col,\n",
    "            \"model\": model_name,\n",
    "            \"ruca_level\": ruca_level,\n",
    "            \"outcome\": outcome,\n",
    "            \"estimate\": float(model.params[index_col]),\n",
    "            \"std_error\": float(model.bse[index_col]),\n",
    "            \"t\": float(model.tvalues[index_col]),\n",
    "            \"p\": float(model.pvalues[index_col]),\n",
    "            \"n\": int(model.nobs),\n",
    "            \"r2\": float(model.rsquared),\n",
    "        })\n",
    "    return pd.DataFrame(tidy)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) LOAD + MERGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading indicators...\")\n",
    "ndvi_df, ndvi_col = load_indicator(NDVI_FILE, NDVI_COL_CANDIDATES)\n",
    "evi_df,  evi_col  = load_indicator(EVI_FILE,  EVI_COL_CANDIDATES)\n",
    "tcc_df,  tcc_col  = load_indicator(TCC_FILE,  TCC_COL_CANDIDATES)\n",
    "\n",
    "print(f\"  NDVI: {NDVI_FILE.name}  col={ndvi_col}  rows={len(ndvi_df):,}\")\n",
    "print(f\"  EVI : {EVI_FILE.name}   col={evi_col}   rows={len(evi_df):,}\")\n",
    "print(f\"  TCC : {TCC_FILE.name}   col={tcc_col}   rows={len(tcc_df):,}\")\n",
    "\n",
    "bio = ndvi_df.merge(evi_df, on=\"GEOID\", how=\"outer\").merge(tcc_df, on=\"GEOID\", how=\"outer\")\n",
    "print(f\"Biophysical merged rows: {len(bio):,}\")\n",
    "\n",
    "print(\"\\nLoading NLCD + PLACES...\")\n",
    "nlcd = pd.read_csv(NLCD_FILE)\n",
    "nlcd_geoid = find_first_existing_col(nlcd, GEOID_CANDIDATES)\n",
    "if nlcd_geoid is None:\n",
    "    raise ValueError(f\"No GEOID in NLCD file. Columns: {list(nlcd.columns)[:50]} ...\")\n",
    "nlcd = nlcd.rename(columns={nlcd_geoid: \"GEOID\"})\n",
    "nlcd[\"GEOID\"] = standardize_geoid(nlcd[\"GEOID\"])\n",
    "\n",
    "places = pd.read_csv(PLACES_FILE)\n",
    "places_geoid = find_first_existing_col(places, GEOID_CANDIDATES)\n",
    "if places_geoid is None:\n",
    "    raise ValueError(f\"No GEOID in PLACES file. Columns: {list(places.columns)[:50]} ...\")\n",
    "places = places.rename(columns={places_geoid: \"GEOID\"})\n",
    "places[\"GEOID\"] = standardize_geoid(places[\"GEOID\"])\n",
    "\n",
    "state_col = find_first_existing_col(places, STATE_COL_CANDIDATES)\n",
    "if state_col is None:\n",
    "    raise ValueError(f\"No state column found in PLACES. Tried: {STATE_COL_CANDIDATES}\")\n",
    "print(f\"Using state col: {state_col}\")\n",
    "\n",
    "ruca_col = find_first_existing_col(places, RUCA_COL_CANDIDATES)\n",
    "if ruca_col:\n",
    "    print(f\"RUCA found in PLACES: {ruca_col}\")\n",
    "else:\n",
    "    ruca_col = None\n",
    "    print(\"No RUCA detected in PLACES (pooled models only).\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) PICK NLCD COLUMNS (READABLE name-based)\n",
    "# ============================================================\n",
    "\n",
    "nlcd_cols = list(nlcd.columns)\n",
    "\n",
    "def pick_cols_by_regex(patterns: List[str]) -> List[str]:\n",
    "    picked = []\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat, flags=re.IGNORECASE)\n",
    "        picked.extend([c for c in nlcd_cols if rx.search(c)])\n",
    "    return picked  # keep order-ish; we will de-dupe later\n",
    "\n",
    "forest_cols   = pick_cols_by_regex([r\"forest\", r\"decidu\", r\"evergr\", r\"mixed\"])\n",
    "shrub_cols    = pick_cols_by_regex([r\"shrub\"])\n",
    "grass_cols    = pick_cols_by_regex([r\"grass\", r\"herb\"])\n",
    "wetland_cols  = pick_cols_by_regex([r\"wetland\", r\"woody\", r\"emerg\"])\n",
    "pasture_cols  = pick_cols_by_regex([r\"pasture\", r\"hay\"])\n",
    "open_cols     = pick_cols_by_regex([r\"open[_ ]?space\", r\"dev.*open\"])\n",
    "\n",
    "NLCD_SELECTED = forest_cols + shrub_cols + grass_cols + wetland_cols + pasture_cols + open_cols\n",
    "\n",
    "# De-duplicate while preserving order\n",
    "NLCD_SELECTED = list(dict.fromkeys(NLCD_SELECTED))\n",
    "\n",
    "if len(NLCD_SELECTED) == 0:\n",
    "    print(\"\\nWARNING: Could not auto-detect NLCD columns by name.\")\n",
    "    print(\"NLCD columns preview (first 150):\")\n",
    "    print(nlcd.columns.tolist()[:150])\n",
    "    raise ValueError(\"Set NLCD_SELECTED manually (no NLCD columns detected).\")\n",
    "\n",
    "print(\"\\nNLCD columns selected:\")\n",
    "for c in NLCD_SELECTED:\n",
    "    print(f\"  - {c}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4) MERGE ALL\n",
    "# ============================================================\n",
    "\n",
    "df = bio.merge(nlcd[[\"GEOID\"] + NLCD_SELECTED], on=\"GEOID\", how=\"left\")\n",
    "df = df.merge(places, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "print(f\"\\nFull merged rows: {len(df):,}\")\n",
    "print(f\"Rows with PLACES present ({PLACES_OUTCOMES[0]}): {pd.to_numeric(df[PLACES_OUTCOMES[0]], errors='coerce').notna().sum():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) SCALE TO [0,1]\n",
    "# ============================================================\n",
    "\n",
    "df = scale_01(df, [ndvi_col, evi_col, tcc_col])\n",
    "df = scale_01(df, NLCD_SELECTED)\n",
    "\n",
    "scaled_preview = [f\"{ndvi_col}_01\", f\"{evi_col}_01\", f\"{tcc_col}_01\"] + [f\"{c}_01\" for c in NLCD_SELECTED]\n",
    "print_basic_diagnostics(df, scaled_preview[:12], title=\"Scaled diagnostics (first 12 scaled cols)\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) BUILD INDICES\n",
    "# ============================================================\n",
    "\n",
    "df = safe_mean(df, [f\"{ndvi_col}_01\", f\"{evi_col}_01\", f\"{tcc_col}_01\"], \"green_eq\")\n",
    "\n",
    "base_components = {f\"{ndvi_col}_01\": 1.0, f\"{evi_col}_01\": 1.0, f\"{tcc_col}_01\": 1.0}\n",
    "nlcd_components = {f\"{c}_01\": 1.0 for c in NLCD_SELECTED}\n",
    "\n",
    "def make_scheme(name: str, w_base: float, w_nlcd: float, extra: Optional[Dict[str, float]] = None):\n",
    "    w: Dict[str, float] = {}\n",
    "    w.update({k: v * w_base for k, v in base_components.items()})\n",
    "    w.update({k: v * w_nlcd for k, v in nlcd_components.items()})\n",
    "    if extra:\n",
    "        w.update(extra)\n",
    "    return name, normalize_weights(w)\n",
    "\n",
    "schemes = [\n",
    "    make_scheme(\"nlcd_eq_expanded\",      1.0, 1.0),\n",
    "    make_scheme(\"nlcd_bio_dominant\",     3.0, 1.0),\n",
    "    make_scheme(\"nlcd_canopy_weighted\",  1.0, 1.0, extra={f\"{tcc_col}_01\": 2.0}),\n",
    "    make_scheme(\"nlcd_vi_weighted\",      1.0, 1.0, extra={f\"{ndvi_col}_01\": 2.0, f\"{evi_col}_01\": 2.0}),\n",
    "    make_scheme(\"nlcd_nlcd_dominant\",    1.0, 3.0),\n",
    "    make_scheme(\"nlcd_conservative\",     4.0, 1.0),\n",
    "    make_scheme(\"nlcd_balanced_canopy\",  2.0, 2.0, extra={f\"{tcc_col}_01\": 1.5}),\n",
    "]\n",
    "\n",
    "for name, w in schemes:\n",
    "    df = build_weighted_index(df, w, outcol=f\"green_{name}\")\n",
    "\n",
    "# PCA PC1\n",
    "pca_inputs = [f\"{ndvi_col}_01\", f\"{evi_col}_01\", f\"{tcc_col}_01\"] + [f\"{c}_01\" for c in NLCD_SELECTED]\n",
    "pca_df = df[pca_inputs].dropna(axis=0, how=\"any\")\n",
    "df[\"green_pca_pc1\"] = np.nan\n",
    "\n",
    "if len(pca_df) >= 1000:\n",
    "    scaler = StandardScaler()\n",
    "    Xz = scaler.fit_transform(pca_df.values)\n",
    "\n",
    "    pca = PCA(n_components=3, random_state=42)\n",
    "    pcs = pca.fit_transform(Xz)\n",
    "\n",
    "    df.loc[pca_df.index, \"green_pca_pc1\"] = pcs[:, 0]\n",
    "\n",
    "    corr = df.loc[pca_df.index, \"green_pca_pc1\"].corr(df.loc[pca_df.index, \"green_eq\"])\n",
    "    if corr is not None and corr < 0:\n",
    "        df.loc[pca_df.index, \"green_pca_pc1\"] *= -1\n",
    "\n",
    "    print(\"\\nPCA summary:\")\n",
    "    print(f\"  N used: {len(pca_df):,}\")\n",
    "    print(f\"  Explained variance ratios: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "    loadings = pd.Series(pca.components_[0], index=pca_inputs).sort_values(ascending=False)\n",
    "    print(\"\\n  PC1 loadings (sorted):\")\n",
    "    print(loadings.to_string())\n",
    "else:\n",
    "    print(\"\\nWARNING: Not enough complete rows for PCA (need >= 1000). PCA skipped.\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) VALIDATION REGRESSIONS\n",
    "# ============================================================\n",
    "\n",
    "missing_outcomes = [o for o in PLACES_OUTCOMES if o not in df.columns]\n",
    "if missing_outcomes:\n",
    "    raise ValueError(f\"Missing PLACES outcomes in merged df: {missing_outcomes}\")\n",
    "\n",
    "index_cols = [\"green_eq\"] + [f\"green_{name}\" for name, _ in schemes] + [\"green_pca_pc1\"]\n",
    "\n",
    "print(\"\\nRunning validation regressions...\")\n",
    "all_results = []\n",
    "for idx in index_cols:\n",
    "    for outcome in PLACES_OUTCOMES:\n",
    "        all_results.append(run_models(df, outcome, idx, state_col, ruca_col))\n",
    "\n",
    "results = pd.concat(all_results, ignore_index=True)\n",
    "results = results.sort_values([\"outcome\", \"index\", \"model\", \"ruca_level\"], na_position=\"last\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TOPLINE RESULTS (first 120 rows)\")\n",
    "print(\"=\"*90)\n",
    "print(results.head(120).to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 8) SAVE OUTPUTS\n",
    "# ============================================================\n",
    "\n",
    "if SAVE_OUTPUTS:\n",
    "    out_indices = OUT_DIR / \"tract_indices_merged.csv\"\n",
    "    out_results = OUT_DIR / \"places_validation_results.csv\"\n",
    "\n",
    "    keep_cols = [\"GEOID\", state_col] + ([ruca_col] if ruca_col else [])\n",
    "    keep_cols += [ndvi_col, evi_col, tcc_col] + NLCD_SELECTED + index_cols + PLACES_OUTCOMES\n",
    "    keep_cols = [c for c in keep_cols if c in df.columns]\n",
    "\n",
    "    df[keep_cols].to_csv(out_indices, index=False)\n",
    "    results.to_csv(out_results, index=False)\n",
    "\n",
    "    print(\"\\nSaved outputs:\")\n",
    "    print(f\"  - {out_indices}\")\n",
    "    print(f\"  - {out_results}\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d50d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ------------------------------------------------------------\n",
    "# purpose: Merge 2020 RUCA tract codes (USDA ERS) into your saved\n",
    "#          tract_indices_merged.csv, then rerun validation regressions.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "ROOT = Path(\"/mnt/sda-21.8/bdevoe/greenspace\")\n",
    "\n",
    "INDICES_FILE = ROOT / \"processed\" / \"index_validation\" / \"tract_indices_merged.csv\"\n",
    "OUT_RESULTS  = ROOT / \"processed\" / \"index_validation\" / \"places_validation_results_with_ruca.csv\"\n",
    "OUT_MERGED   = ROOT / \"processed\" / \"index_validation\" / \"tract_indices_merged_with_ruca.csv\"\n",
    "\n",
    "RUCA_LOCAL   = ROOT / \"resources\" / \"RUCA-codes-2020-tract.csv\"\n",
    "\n",
    "PLACES_OUTCOMES = [\n",
    "    \"OBESITY_CrudePrev\",\n",
    "    \"LPA_CrudePrev\",\n",
    "    \"MHLTH_CrudePrev\",\n",
    "    \"PHLTH_CrudePrev\",\n",
    "]\n",
    "\n",
    "STATE_COL = \"StateAbbr\"\n",
    "RUCA_COL_OUT = \"RUCA10\"\n",
    "MIN_STRAT_N = 300\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def standardize_geoid(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    return s.str.zfill(11)\n",
    "\n",
    "def find_first_existing_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, encoding=\"latin1\", on_bad_lines=\"skip\")\n",
    "\n",
    "def run_models(df: pd.DataFrame, outcome: str, index_col: str, state_col: str, ruca_col: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    d = df[[outcome, index_col, state_col, ruca_col]].copy()\n",
    "    d = d.dropna(subset=[outcome, index_col, state_col, ruca_col])\n",
    "\n",
    "    m1 = smf.ols(f\"{outcome} ~ {index_col}\", data=d).fit(cov_type=\"HC1\")\n",
    "    rows.append((\"pooled_unadjusted\", None, m1))\n",
    "\n",
    "    m2 = smf.ols(f\"{outcome} ~ {index_col} + C({state_col})\", data=d).fit(cov_type=\"HC1\")\n",
    "    rows.append((\"pooled_stateFE\", None, m2))\n",
    "\n",
    "    m3 = smf.ols(f\"{outcome} ~ {index_col} + C({state_col}) + C({ruca_col})\", data=d).fit(cov_type=\"HC1\")\n",
    "    rows.append((\"pooled_stateFE_rucaFE\", None, m3))\n",
    "\n",
    "    for r in sorted(d[ruca_col].dropna().unique()):\n",
    "        dr = d[d[ruca_col] == r].copy()\n",
    "        if len(dr) < MIN_STRAT_N:\n",
    "            continue\n",
    "        ms = smf.ols(f\"{outcome} ~ {index_col} + C({state_col})\", data=dr).fit(cov_type=\"HC1\")\n",
    "        rows.append((\"ruca_strat_stateFE\", r, ms))\n",
    "\n",
    "    out = []\n",
    "    for model_name, ruca_level, m in rows:\n",
    "        if index_col not in m.params.index:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"index\": index_col,\n",
    "            \"model\": model_name,\n",
    "            \"ruca_level\": ruca_level,\n",
    "            \"outcome\": outcome,\n",
    "            \"estimate\": float(m.params[index_col]),\n",
    "            \"std_error\": float(m.bse[index_col]),\n",
    "            \"t\": float(m.tvalues[index_col]),\n",
    "            \"p\": float(m.pvalues[index_col]),\n",
    "            \"n\": int(m.nobs),\n",
    "            \"r2\": float(m.rsquared),\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# ============================================================\n",
    "# 1) LOAD RUCA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Loading RUCA: {RUCA_LOCAL}\")\n",
    "ruca = read_csv_robust(RUCA_LOCAL)\n",
    "\n",
    "# Your RUCA file has TractFIPS20 and TractFIPS23; prefer 2020 tracts\n",
    "geoid_col = find_first_existing_col(\n",
    "    ruca,\n",
    "    [\"TractFIPS20\", \"TractFIPS23\", \"TractFIPS\", \"GEOID\", \"geoid\", \"FIPS\", \"fips\"]\n",
    ")\n",
    "if geoid_col is None:\n",
    "    raise ValueError(f\"Could not find a GEOID/FIPS column in RUCA file. Columns: {list(ruca.columns)[:60]}\")\n",
    "\n",
    "primary_col = find_first_existing_col(\n",
    "    ruca,\n",
    "    [\"PrimaryRUCA\", \"primaryruca\", \"Primary RUCA\", \"Primary_RUCA\"]\n",
    ")\n",
    "if primary_col is None:\n",
    "    raise ValueError(f\"Could not find PrimaryRUCA in RUCA file. Columns: {list(ruca.columns)[:60]}\")\n",
    "\n",
    "ruca = ruca.rename(columns={geoid_col: \"GEOID\", primary_col: RUCA_COL_OUT}).copy()\n",
    "ruca[\"GEOID\"] = standardize_geoid(ruca[\"GEOID\"])\n",
    "ruca[RUCA_COL_OUT] = pd.to_numeric(ruca[RUCA_COL_OUT], errors=\"coerce\")\n",
    "ruca = ruca[ruca[RUCA_COL_OUT].between(1, 10)].copy()\n",
    "\n",
    "print(\"\\nRUCA loaded:\")\n",
    "print(\"  rows:\", len(ruca))\n",
    "print(\"  RUCA counts:\")\n",
    "print(ruca[RUCA_COL_OUT].value_counts().sort_index().to_string())\n",
    "\n",
    "# ============================================================\n",
    "# 2) LOAD INDICES + MERGE RUCA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\nLoading indices: {INDICES_FILE}\")\n",
    "df = pd.read_csv(INDICES_FILE)\n",
    "df[\"GEOID\"] = standardize_geoid(df[\"GEOID\"])\n",
    "\n",
    "if STATE_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected {STATE_COL} in indices file. Found: {df.columns.tolist()[:60]}\")\n",
    "\n",
    "df = df.merge(ruca[[\"GEOID\", RUCA_COL_OUT]], on=\"GEOID\", how=\"left\")\n",
    "\n",
    "print(\"\\nMerged RUCA into indices:\")\n",
    "print(\"  total rows:\", len(df))\n",
    "print(\"  rows with RUCA:\", df[RUCA_COL_OUT].notna().sum())\n",
    "\n",
    "df.to_csv(OUT_MERGED, index=False)\n",
    "print(\"Saved merged indices+RUCA:\", OUT_MERGED)\n",
    "\n",
    "# ============================================================\n",
    "# 3) RUN REGRESSIONS WITH RUCA\n",
    "# ============================================================\n",
    "\n",
    "missing_outcomes = [o for o in PLACES_OUTCOMES if o not in df.columns]\n",
    "if missing_outcomes:\n",
    "    raise ValueError(f\"Missing PLACES outcomes in merged df: {missing_outcomes}\")\n",
    "\n",
    "index_cols = [c for c in df.columns if c.startswith(\"green_\")]\n",
    "if not index_cols:\n",
    "    raise ValueError(\"No index columns found (expected columns starting with 'green_').\")\n",
    "\n",
    "print(\"\\nRunning validation regressions WITH RUCA...\")\n",
    "all_res = []\n",
    "for idx in index_cols:\n",
    "    for outcome in PLACES_OUTCOMES:\n",
    "        all_res.append(run_models(df, outcome, idx, STATE_COL, RUCA_COL_OUT))\n",
    "\n",
    "results = pd.concat(all_res, ignore_index=True)\n",
    "results = results.sort_values([\"outcome\", \"index\", \"model\", \"ruca_level\"], na_position=\"last\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"TOPLINE RESULTS (first 120 rows) â€” WITH RUCA\")\n",
    "print(\"=\"*90)\n",
    "print(results.head(120).to_string(index=False))\n",
    "\n",
    "results.to_csv(OUT_RESULTS, index=False)\n",
    "print(\"\\nSaved:\", OUT_RESULTS)\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
